{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DLwEgO_gn2ZK",
        "outputId": "e323a180-38a4-4471-96aa-0760ac1ee2cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.55.3\n",
            "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.55.3) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.55.3) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.55.3) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.55.3) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.55.3) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai==1.55.3) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai==1.55.3) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai==1.55.3) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai==1.55.3) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.55.3) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.55.3) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.55.3) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (0.4.1)\n",
            "Downloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.86.0\n",
            "    Uninstalling openai-1.86.0:\n",
            "      Successfully uninstalled openai-1.86.0\n",
            "Successfully installed openai-1.55.3\n",
            "Collecting beautifulsoup4==4.12.3\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4==4.12.3) (2.7)\n",
            "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: beautifulsoup4\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.4\n",
            "    Uninstalling beautifulsoup4-4.13.4:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.4\n",
            "Successfully installed beautifulsoup4-4.12.3\n",
            "Collecting requests==2.31.0\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (2025.6.15)\n",
            "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: requests\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed requests-2.31.0\n",
            "Collecting deeplake==3.9.18\n",
            "  Downloading deeplake-3.9.18.tar.gz (608 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.9/608.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy<2.0 (from deeplake==3.9.18)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow~=10.2.0 (from deeplake==3.9.18)\n",
            "  Downloading pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting boto3 (from deeplake==3.9.18)\n",
            "  Downloading boto3-1.38.39-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from deeplake==3.9.18) (8.2.1)\n",
            "Collecting pathos (from deeplake==3.9.18)\n",
            "  Downloading pathos-0.3.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting humbug>=0.3.1 (from deeplake==3.9.18)\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deeplake==3.9.18) (4.67.1)\n",
            "Collecting lz4 (from deeplake==3.9.18)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.11/dist-packages (from deeplake==3.9.18) (2.10.1)\n",
            "Collecting aioboto3>=10.4.0 (from deeplake==3.9.18)\n",
            "  Downloading aioboto3-14.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from deeplake==3.9.18) (1.6.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from deeplake==3.9.18) (2.11.7)\n",
            "Collecting libdeeplake==0.0.138 (from deeplake==3.9.18)\n",
            "  Downloading libdeeplake-0.0.138-cp311-cp311-manylinux2014_x86_64.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from libdeeplake==0.0.138->deeplake==3.9.18) (0.3.7)\n",
            "Collecting aiobotocore==2.22.0 (from aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18)\n",
            "  Downloading aiobotocore-2.22.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: aiofiles>=23.2.1 in /usr/local/lib/python3.11/dist-packages (from aioboto3>=10.4.0->deeplake==3.9.18) (24.1.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.2 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (3.11.15)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting botocore<1.37.4,>=1.37.2 (from aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18)\n",
            "  Downloading botocore-1.37.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (2.9.0.post0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (6.4.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (1.17.2)\n",
            "Collecting boto3 (from deeplake==3.9.18)\n",
            "  Downloading boto3-1.37.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->deeplake==3.9.18)\n",
            "  Downloading s3transfer-0.11.5-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from humbug>=0.3.1->deeplake==3.9.18) (2.31.0)\n",
            "Collecting ppft>=1.7.7 (from pathos->deeplake==3.9.18)\n",
            "  Downloading ppft-1.7.7-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill (from libdeeplake==0.0.138->deeplake==3.9.18)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pox>=0.3.6 (from pathos->deeplake==3.9.18)\n",
            "  Downloading pox-0.3.6-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.18 (from pathos->deeplake==3.9.18)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->deeplake==3.9.18) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->deeplake==3.9.18) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->deeplake==3.9.18) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->deeplake==3.9.18) (0.4.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.37.4,>=1.37.2->aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (2.4.0)\n",
            "INFO: pip is looking at multiple versions of s3transfer to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->deeplake==3.9.18)\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading s3transfer-0.11.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->humbug>=0.3.1->deeplake==3.9.18) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->humbug>=0.3.1->deeplake==3.9.18) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->humbug>=0.3.1->deeplake==3.9.18) (2025.6.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore==2.22.0->aiobotocore[boto3]==2.22.0->aioboto3>=10.4.0->deeplake==3.9.18) (1.17.0)\n",
            "Downloading libdeeplake-0.0.138-cp311-cp311-manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioboto3-14.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading aiobotocore-2.22.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.37.3-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathos-0.3.4-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.3-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pox-0.3.6-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.11.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.9.18-py3-none-any.whl size=731795 sha256=fa66de33943562504cdd85fb011d4bbb4d9c2aac20e08cc82a49de1426ce7b80\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/66/79/b6e76d46b737f60be7542e1f4650141ef3cb09fdba9ca22aaa\n",
            "Successfully built deeplake\n",
            "Installing collected packages: ppft, pox, pillow, numpy, lz4, jmespath, dill, aioitertools, multiprocess, libdeeplake, humbug, botocore, s3transfer, pathos, aiobotocore, boto3, aioboto3, deeplake\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.7\n",
            "    Uninstalling dill-0.3.7:\n",
            "      Successfully uninstalled dill-0.3.7\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.15\n",
            "    Uninstalling multiprocess-0.70.15:\n",
            "      Successfully uninstalled multiprocess-0.70.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "datasets 2.14.4 requires dill<0.3.8,>=0.3.0, but you have dill 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aioboto3-14.3.0 aiobotocore-2.22.0 aioitertools-0.12.0 boto3-1.37.3 botocore-1.37.3 deeplake-3.9.18 dill-0.4.0 humbug-0.3.2 jmespath-1.0.1 libdeeplake-0.0.138 lz4-4.4.4 multiprocess-0.70.18 numpy-1.26.4 pathos-0.3.4 pillow-10.2.0 pox-0.3.6 ppft-1.7.7 s3transfer-0.11.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              },
              "id": "ef6ab12010df45c38438911c03bcca8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.2.8) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==1.55.3\n",
        "!pip install beautifulsoup4==4.12.3\n",
        "!pip install requests==2.31.0\n",
        "try:\n",
        "  import deeplake\n",
        "except:\n",
        "  !pip install deeplake==3.9.18\n",
        "  import deeplake"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import textwrap\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import textwrap\n",
        "try:\n",
        "  import fitz\n",
        "except:\n",
        "  !pip install pymupdf\n",
        "\n",
        "# for similarity search\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B7RVbtyoeCP",
        "outputId": "407cc9dd-4ce6-4f3d-d933-b5f19870a48a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set APIs keys into enviroment variables\n",
        "os.environ['OPEN_API_KEY'] = 'OPEN_API_KEY'\n",
        "os.environ['ACTIVELOOP_TOKEN'] = 'ACTIVELOOP_TOKEN'\n",
        "\n",
        "openai.api_key = os.getenv('OPEN_API_KEY')\n"
      ],
      "metadata": {
        "id": "V8ExqEBGoqYU"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the public data for create the retrival store"
      ],
      "metadata": {
        "id": "wuRDMUNsqW2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor():\n",
        "\n",
        "  def __init__(self, urls, doc_file_path='doc_file_path', save_to='filepath'):\n",
        "    self.urls = urls\n",
        "    self.save_to = 'knowledgebase.txt'\n",
        "    self.doc_file_path = doc_file_path\n",
        "\n",
        "\n",
        "  def clean_text(self, content):\n",
        "      content = re.sub(r'\\d+', '', content)\n",
        "      return content\n",
        "\n",
        "  def retrieve_from_url(self, url):\n",
        "      response = requests.get(url)\n",
        "      soap = BeautifulSoup(response.text, 'html.parser')\n",
        "      content = soap.find('div', {'class': 'mw-parser-output'})\n",
        "\n",
        "      # Extract and clean the content\n",
        "      text = content.get_text(separator=' ', strip=True)\n",
        "      text = self.clean_text(content)\n",
        "      return text\n",
        "\n",
        "  def retrieve_from_pdf(self, file_name):\n",
        "      doc = fitz.open(filename=filename)\n",
        "      return '\\n'.join(page.get_text() for page in doc)\n",
        "\n",
        "  def process_content(self):\n",
        "    with open('knowledgebase.txt', 'w') as file:\n",
        "      for url in self.urls:\n",
        "        content_from_web = self.retrieve_from_url(url)\n",
        "\n",
        "      content_from_pdf = self.retrieve_from_pdf(self.doc_file_path)\n",
        "\n",
        "      knowledge_content = content_from_pdf + '\\n' + content_from_web\n",
        "\n",
        "      file.write(knowledge_content + '\\n')"
      ],
      "metadata": {
        "id": "6OPNe7_bt6mh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = ['https://url1', 'https://url2']\n",
        "\n",
        "data_processor = DataProcessor(urls)\n",
        "\n",
        "data_processor.process_content()\n"
      ],
      "metadata": {
        "id": "scPVvkxt1t8D"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding and vector store"
      ],
      "metadata": {
        "id": "gncP4JtXuE7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deeplake.core.vectorstore.deeplake_vectorstore import VectorStore\n",
        "import deeplake\n",
        "import deeplake.util\n",
        "\n",
        "chunk_size = 1000\n",
        "add_to_vector_store = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICwlvxXAucjL",
        "outputId": "0dc5eb5a-003a-40c1-e7c8-1c98b209f9f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.2.8) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStoreBuilder():\n",
        "  def __init__(self,texts, model_name = 'text-embedding-3-small',add_to_vector_store=False, chunk_size=200,vector_store_path = '<vector_store_path'):\n",
        "    self.model_name = model_name\n",
        "    self.texts = 'knowledgebase.txt'\n",
        "    self.vector_store_path = vector_store_path\n",
        "\n",
        "  def is_vector_store_exist(self):\n",
        "    try:\n",
        "      vector_store = VectorStore(self.vector_store_path)\n",
        "      print('Vector store exists')\n",
        "    except:\n",
        "      print('Vector store does not exists')\n",
        "      self.add_to_vector_store = True\n",
        "\n",
        "  def embedding_fn(self):\n",
        "    if isinstance(self.texts, str):\n",
        "      texts = [self.texts]\n",
        "      texts = [text.replace('\\n','') for text in texts]\n",
        "      return [data.embedding for data in openai.embeddings.create(input=texts, model=model).data]\n",
        "\n",
        "  def store_to_vector_db(self):\n",
        "    if self.add_to_vector_store:\n",
        "      with open(self.texts, 'r') as file:\n",
        "        text = file.read()\n",
        "        chunk_size = self.chunk_size\n",
        "        chunked_text = [text[i: i+chunk_size] for i in range(0,len(text), chunk_size)] # This is the straight way to do chunking but it may loose context hence we can do overlapping\n",
        "\n",
        "    vector_store.add(\n",
        "        text = chunked_text,\n",
        "        embedding_function= self.embedding_fn,\n",
        "        embedding_data= chunked_text,\n",
        "        metadata= [{'source': 'knowledgebase.txt'}] * len(chunked_text)\n",
        ")\n"
      ],
      "metadata": {
        "id": "H58ZMZX_uCGw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store_builder = VectorStoreBuilder(texts='knowledgebase.txt')\n",
        "vector_store_builder.is_vector_store_exist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INZ7YSte-QMW",
        "outputId": "1703ebf3-61d3-40f7-c323-1b0c673df8d8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store does not exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "yzW3shpiAqDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAG():\n",
        "  def __init__(self, vector_store_path = 'vector_store_path'):\n",
        "    try:\n",
        "      self.vector_store = deeplake.load(vector_store_path)\n",
        "      print(\"Vector DB exist\")\n",
        "    except:\n",
        "      print(\"Vector DB does not exist\")\n",
        "      self.vector_store = deeplake.empty  # create empty but no use\n",
        "\n",
        "  def build_prompt(self,user_prompt):\n",
        "    search_results = self.vector_store.search(user_prompt, embedding_function=embedding_fn)\n",
        "    best_similarity_result = search_results['text'][0]\n",
        "    return f'{user_prompt} {best_similarity_result}'\n"
      ],
      "metadata": {
        "id": "n_b5m0lv47Ox"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag = RAG(vector_store_path = 'vector_store_path')\n",
        "agumented_prompt = rag.build_prompt(\"user_prompt\")\n",
        "text_wrapper = textwrap.TextWrapper(width=100)\n",
        "print(text_wrapper.fill(agumented_prompt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSWg52Hb7wtE",
        "outputId": "34214b08-c830-4904-a06c-173be057c6e1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector DB does not exist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT (Generator)"
      ],
      "metadata": {
        "id": "PAhF3TBVAsyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call GPT with augumented prompt\n",
        "client = OpenAI()\n",
        "def call_gpt(text):\n",
        "  text_input = ''.join(text)\n",
        "  prompt = f\"Please summarize or elobrate on the following content \\n {text_input}\"\n",
        "  try:\n",
        "    response = client.chat.complentions.create(\n",
        "        model = 'model_name',\n",
        "        messages = [\n",
        "            {'role': 'system', 'content': \"You are the AI assistance.\"},\n",
        "            {'role': 'assistant', 'content': \"Please provide the answer based on input\"},\n",
        "            {'role': 'user', 'content': prompt}\n",
        "        ],\n",
        "        temperature = 0.1\n",
        "    )\n",
        "    return response.choice[0].message.content.strip()\n",
        "\n",
        "  except Exception as e:\n",
        "    return str(e)\n"
      ],
      "metadata": {
        "id": "sxAlhyQrAdNq"
      },
      "execution_count": 64,
      "outputs": []
    }
  ]
}